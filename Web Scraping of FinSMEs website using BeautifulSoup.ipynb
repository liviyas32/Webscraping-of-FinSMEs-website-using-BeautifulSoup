{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c937143d",
   "metadata": {},
   "source": [
    "<b>Introduction</b></br>\n",
    "The dynamic landscape of financial markets demands real-time insights to empower investors in making informed decisions. In this context, Finsmes emerges as a pivotal resource, providing a wealth of information and news crucial for investors navigating the intricate world of finance. Finsmes, a prominent financial news platform, serves as a beacon for investors seeking to stay abreast of market trends, investments, startups, venture capital, emerging opportunities, and critical developments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63921fab",
   "metadata": {},
   "source": [
    "<b>Rationale</b></br>\n",
    "The rapid pace of financial markets and the wealth of information available make it imperative for investors to have timely, consolidated, and actionable insights. Recognizing the need for a comprehensive tool tailored to extract key financial news data, the rationale behind this project stems from the challenges investors face in efficiently gathering and analyzing relevant information from platforms like Finsmes.\n",
    "\n",
    "Financial news platforms, such as Finsmes, are rich sources of market trends and investment opportunities. However, the manual extraction of data is both time-consuming and prone to errors, hindering investors from accessing real-time information crucial for decision-making. The rationale for this project, therefore, lies in the necessity to automate the extraction process, providing a robust and efficient solution to empower investors with a consolidated dataset.\n",
    "\n",
    "By addressing the manual data collection bottleneck, this project seeks to streamline the information retrieval process, ensuring accuracy and consistency in the extracted dataset. The rationale underscores the project's commitment to enhancing the accessibility of financial news data, allowing investors to focus on analyzing trends and patterns rather than spending valuable time on manual data compilation.\n",
    "\n",
    "Ultimately, the rationale for this project is grounded in the conviction that an automated web scraping tool for Finsmes will not only save time but will also provide investors with a reliable resource. This resource will offer a consolidated dataset comprising news links, published dates, headlines, and content, enabling investors to stay informed, identify emerging trends, and make well-informed decisions aligned with their investment strategies. The overarching goal is to empower investors to navigate the complexities of the financial landscape with heightened confidence and agility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6028d5",
   "metadata": {},
   "source": [
    "<b>Problem Statement</b></br>\n",
    "Investors face the challenge of efficiently sourcing and processing vast amounts of financial news data scattered across various platforms. The manual collection of such data is time-consuming and prone to errors. The project aimed to address this challenge by developing a web scraping solution tailored to extract relevant data from Finsmes, thereby mitigating the data collection bottleneck and providing a consolidated, structured dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fceaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eab2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the user data  - country and n\n",
    "# country - select the country of interest from the list [All, USA, UK, GERMANY, FRANCE, CANADA, INDIA, ITALY]\n",
    "# n - select the number of pages of interest \n",
    "country = input('Select the country of your interest from [ALL, USA, UK, GERMANY, FRANCE, CANADA, INDIA, ITALY] : ').lower()\n",
    "n = int(input('Enter the number of pages of your interest to scrape: '))\n",
    "\n",
    "try:\n",
    "    root = 'https://www.finsmes.com/'\n",
    "    # requests for access\n",
    "    source = requests.get(root)\n",
    "    source.raise_for_status() # helps in capturing error if link doesnt work\n",
    "    \n",
    "    \n",
    "    # parse html content\n",
    "    home_page = bs(source.text, 'html.parser')\n",
    "    # extracting mainpage links for each country\n",
    "    usa = home_page.find('li', id = 'menu-item-76541').find('a').get('href')\n",
    "    uk = home_page.find('li', id = 'menu-item-76542').find('a').get('href')\n",
    "    germany = home_page.find('li', id = 'menu-item-76543').find('a').get('href')\n",
    "    france = home_page.find('li', id = 'menu-item-76547').find('a').get('href')\n",
    "    canada = home_page.find('li', id = 'menu-item-76544').find('a').get('href')\n",
    "    india = home_page.find('li', id = 'menu-item-76545').find('a').get('href')\n",
    "    italy = home_page.find('li', id = 'menu-item-76546').find('a').get('href')\n",
    "    root = 'https://www.finsmes.com/'\n",
    "    \n",
    "    \n",
    "    # create a dataframe to store the extracted data\n",
    "    df = pd.DataFrame(columns = ['Article URL', 'Published Date', 'Headline', 'Content'])\n",
    "    \n",
    "    # create a condition that calls the sub-function based on user-input\n",
    "    if country == 'all':\n",
    "        investment_news(root,n)\n",
    "    elif country == 'usa':\n",
    "        investment_news(usa,n)\n",
    "    elif country == 'uk':\n",
    "        investment_news(uk,n)\n",
    "    elif country == 'germany':\n",
    "        investment_news(germany,n)\n",
    "    elif country == 'france':\n",
    "        investment_news(france,n)\n",
    "    elif country == 'canada':\n",
    "        investment_news(canada,n)\n",
    "    elif country == 'india':\n",
    "        investment_news(india,n)\n",
    "    elif country == 'italy':\n",
    "        investment_news(italy,n)\n",
    "    else:\n",
    "        print('Kindly enter valid country!')\n",
    "        \n",
    "    \n",
    "    # store/output the df in excel file\n",
    "    df.to_excel('finsmes_investment_news.xlsx')\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690907d4",
   "metadata": {},
   "source": [
    "Select the country of your interest from [ALL, USA, UK, GERMANY, FRANCE, CANADA, INDIA, ITALY] : usa\n",
    "\n",
    "Enter the number of pages of your interest to scrape: 3322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a sub-function that extracts all the urls from a given page\n",
    "def investment_news(option, n):\n",
    "    # requests for access\n",
    "    main_page = requests.get(option)\n",
    "    \n",
    "    # create an empty list to store the extracted urls\n",
    "    news_links = []\n",
    "    \n",
    "    # loops for n pages from main page\n",
    "    for i in range(0,n):\n",
    "        \n",
    "        # parse main_page\n",
    "        main_page = bs(main_page.text, 'html.parser')\n",
    "        \n",
    "        # finds all header tags with class entry-title\n",
    "        links = main_page.find_all('h2', class_ = 'entry-title')\n",
    "        \n",
    "        # loops for each link in links\n",
    "        for link in links:\n",
    "            \n",
    "            # extracts href content from a tag\n",
    "            if link.find('a') is not None:\n",
    "                url = link.find('a')['href']\n",
    "                \n",
    "                #appends extracted href in news_links\n",
    "                news_links.append(url)\n",
    "                \n",
    "        # calls main function to extract the required data\n",
    "        news_parse(news_links)\n",
    "        \n",
    "        # finds next page link\n",
    "        next_page = main_page.find('div', class_ = 'nav-previous').find('a')['href']\n",
    "        \n",
    "        # requests access for next_page\n",
    "        main_page = requests.get(next_page)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963507ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a main function that extracts the required data from a given article link\n",
    "# url, published_date, news_headline, content are the required data in this project\n",
    "def news_parse(news_links):\n",
    "    # loops until all urls in the given list is parsed\n",
    "    for i in range(0, len(news_links)):\n",
    "        #requests for access\n",
    "        url.requests.get(news_links[i])\n",
    "        # wait for 1 second\n",
    "        time.sleep(1)\n",
    "        \n",
    "        #parsing html content\n",
    "        soup = bs(url.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        # published date attribute using try and except block\n",
    "        try:\n",
    "            published_date = soup.find('div', class_ = 'author-date').find('time', class_ = 'entry-date published').text\n",
    "        except:\n",
    "            published_Date = soup.find('div', class_ = 'author-date').find('time', class_ = 'entry-date published updated').text\n",
    "            \n",
    "        \n",
    "        # headline attribute\n",
    "        try:\n",
    "            headline = soup.find('header', class_ = 'entry-header').find('h1', class_ = 'enrty-title').text\n",
    "        except:\n",
    "            headline = np.NaN\n",
    "            \n",
    "        # content attribute\n",
    "        try:\n",
    "            content = soup.find('div', class_ = 'entry-content').find('p').text\n",
    "        except:\n",
    "            content = np.NaN\n",
    "        \n",
    "        \n",
    "        # creatinga dictionary using extracted data\n",
    "        data = dict()\n",
    "        data.update({'Article URL' : news_links[i], 'Published Date' : published_date, 'Headline' : headline, 'Content' : content})\n",
    "        \n",
    "        \n",
    "        # append the data into the original df\n",
    "        global df\n",
    "        df = df.append(data, ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
